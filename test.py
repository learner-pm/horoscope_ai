import os
import pickle
import jieba

# åŠ è½½ Tokenizer
tokenizer_path = os.path.join(os.getcwd(), "models", "tokenizer.pkl")
if not os.path.exists(tokenizer_path):
    raise FileNotFoundError(f"Tokenizer æ–‡ä»¶ {tokenizer_path} ä¸å­˜åœ¨ï¼")

with open(tokenizer_path, "rb") as f:
    tokenizer = pickle.load(f)

# âœ… æµ‹è¯•æ•°æ®ï¼šåŒ…å«æ˜Ÿåº§å’Œå…³é”®å­—
test_texts = ["ç™½ç¾Šåº§çš„æ€§æ ¼å¾ˆæœ‰é­…åŠ›", "é‡‘ç‰›åº§çš„è¿åŠ¿ä¸é”™", "å¹¸è¿è‰²æ˜¯çº¢è‰²", "å¤©èåº§çš„äº‹ä¸šå……æ»¡æŒ‘æˆ˜"]
test_tokens = [list(jieba.cut(text)) for text in test_texts]  # jieba åˆ†è¯
test_sequences = tokenizer.texts_to_sequences([" ".join(tokens) for tokens in test_tokens])  # è½¬æ¢ä¸º ID

# âœ… æ‰“å°è½¬æ¢ç»“æœ
print("=" * 60)
print("ğŸš€  æµ‹è¯• Tokenizer æ˜¯å¦æ­£ç¡®è½¬æ¢æ–‡æœ¬ ğŸš€")
print("=" * 60)

for text, tokens, seq in zip(test_texts, test_tokens, test_sequences):
    print(f"åŸå§‹æ–‡æœ¬: {text}")
    print(f"åˆ†è¯ç»“æœ: {tokens}")
    print(f"è½¬æ¢ä¸º ID: {seq}")
    print("-" * 50)

# âœ… æ£€æŸ¥ OOV
oov_index = tokenizer.word_index.get("<OOV>", None)
if oov_index:
    print(f"<OOV> è¯çš„ ID: {oov_index}")

for i, seq in enumerate(test_sequences):
    if not seq:
        print(f"âš  è­¦å‘Š: '{test_texts[i]}' æ— æ³•è½¬æ¢ä¸º IDï¼Œå¯èƒ½æ˜¯ OOVï¼")
    elif oov_index in seq:
        print(f"âš  è­¦å‘Š: '{test_texts[i]}' è½¬æ¢ååŒ…å« OOVï¼")

print("=" * 60)
print("âœ…  æµ‹è¯•å®Œæˆï¼è¯·æ£€æŸ¥è½¬æ¢ç»“æœï¼")
print("=" * 60)


# âœ… æ£€æŸ¥ OOV è¯æ˜¯å¦åœ¨ tokenizer é‡Œ
check_words = ["ç™½ç¾Šåº§", "æ€§æ ¼", "é­…åŠ›", "å¹¸è¿è‰²", "çº¢è‰²", "æŒ‘æˆ˜"]
for word in check_words:
    print(f"'{word}' ID:", tokenizer.word_index.get(word, "âŒ OOV"))


# âœ… ç»Ÿè®¡ tokenizer è¯è¡¨å¤§å°
print("è¯è¡¨å¤§å°:", len(tokenizer.word_index))

# âœ… æŸ¥çœ‹æœ€å¸¸è§çš„ 50 ä¸ªè¯
top_words = sorted(tokenizer.word_index.items(), key=lambda x: x[1])[:50]
print("å‰ 50 ä¸ªå•è¯:", top_words)